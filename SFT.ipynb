{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91K3UHYpABZQ",
    "outputId": "554bf622-da8e-4ddf-8519-6e4aa9a0096c"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TPGlAyONAcBY",
    "outputId": "83a1485e-9769-4b7d-9a22-31c146523151"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets peft accelerate bitsandbytes sentencepiece scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NjTeEvpmTdmG",
    "outputId": "3b488eb6-6a70-40e6-c21a-7915ff8a1c09"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Replace with actual TinyLlama HF repo or path\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model with 4-bit quantization for efficiency if desired\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5kqKVRveyzw"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"/content/drive/MyDrive/NvidiaDocumentationQandApairs.csv\")\n",
    "\n",
    "# Optional: Clean text as you did earlier (lowercase, remove non-alphanumeric)\n",
    "\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^A-Za-z0-9\\s]+', '', text)\n",
    "    return text\n",
    "\n",
    "data['question'] = data['question'].apply(clean_text)\n",
    "data['answer'] = data['answer'].apply(clean_text)\n",
    "\n",
    "# Create a new column with prompt+answer concatenated\n",
    "def build_prompt(q, a):\n",
    "    return f\"Question: {q}\\nAnswer: {a}\"\n",
    "\n",
    "data['input_text'] = data.apply(lambda row: build_prompt(row['question'], row['answer']), axis=1)\n",
    "\n",
    "# Split train/val/test\n",
    "train_df, test_df = train_test_split(data, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds = Dataset.from_pandas(val_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "96452c7f60364c9c8d9870a3ec553a83",
      "bdddac3dfbad4d3a8f120690057c81d9",
      "84153d2e95d24c149c28abad4eda7ad7",
      "63a58c9510614a3387e83184aad22528",
      "d6321e09f272425592593fe5cd6eea43",
      "27133929d3644bcbb80e10b7adebfee2",
      "7f753c24599a4a80abb65d419958d6e7",
      "62fc7b8fc13b41008f69df12a2e5b04e",
      "c29a8c74ebcb4cc3a358c841efb56ab7",
      "5a2962d2be664cfdb505d44f0879e6a0",
      "6690c0f4889d4311aa4b10f7fc7c38ce",
      "47fa5f9ffd5f47a2b736f84cbda5ff3a",
      "81f6b55cbcdb427bbbe2cbf739b9a7b3",
      "97ba1926effd490c8cf62cf5879a4e23",
      "0cd16fad1fde4c61a8d6cebdae6bbd9e",
      "8230377db90a49c4bcaf502afc80e981",
      "e3d3eea692494234925c021f67199827",
      "f949f1b236e9442db5ab02e402dc3a3d",
      "b6ad97a9b1a046a69888812eeb355d60",
      "f97ee9803e6747d6bb93c02cd91702d5",
      "9b00b18db2294aaca00074852ba1bff5",
      "a8cc7bdf5a9d4d48866c6b6a3b6a9ce9",
      "3680442c2223421ba79e4f231c952165",
      "d0ff7fdee60349f6847b2b68688ed2d0",
      "c2c9a264eade4d4e98af82e46f79936a",
      "597b8c6a013a4f859dbe87bfb5f1062f",
      "59daccda9fbb4d1e89991c5d49a65632",
      "eb39de243eac4feb8278e647329e5dbe",
      "b659c6bbc9d44cfbb567fd30bd8fab77",
      "62923a5e35254991b8dd095704f3f212",
      "0863c0fd1a5e41e6aa178ff553f31ecf",
      "d603911f8864487badd72d85534e5629",
      "145ce2c9e84c487b9e0e5ae2e4c927a1"
     ]
    },
    "id": "VHlqSi9pe9c0",
    "outputId": "6a86d141-3788-432d-a625-7ced3bdb9845"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Tokenize full prompt+answer text\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"input_text\"],\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    # Labels are same as input_ids (causal LM)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "train_tokenized = train_ds.map(tokenize_function, batched=True, remove_columns=train_ds.column_names)\n",
    "val_tokenized = val_ds.map(tokenize_function, batched=True, remove_columns=val_ds.column_names)\n",
    "test_tokenized = test_ds.map(tokenize_function, batched=True, remove_columns=test_ds.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9zYVupe_fAou",
    "outputId": "639e73c5-56ae-4a54-deac-86540b7f6759"
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,                 # smaller rank for tiny model\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # typical for LLaMA; confirm correct module names for TinyLlama\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "def print_trainable_params(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    all_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable params: {trainable_params} / {all_params} ({100*trainable_params/all_params:.2f}%)\")\n",
    "\n",
    "print_trainable_params(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ONYdcMxtfEha",
    "outputId": "10038f9d-e1de-4e5a-dd84-71d524f4cba8"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/tinyllama-lora-sft\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=4,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    label_names=[\"labels\"],\n",
    "\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # causal LM\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "A1BtgJJTfY6o",
    "outputId": "91e799e3-4845-4210-9ae3-0bb4765e0be4"
   },
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wFF9f4QfpcN7",
    "outputId": "349929f6-2047-426b-b1d7-226977ad8870"
   },
   "outputs": [],
   "source": [
    "output_dir = \"/content/drive/MyDrive/tinyllama-lora-sft-tuned-model\"\n",
    "\n",
    "# Save the PEFT model (includes LoRA weights)\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "\n",
    "# Save the tokenizer as well\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSCVbhVjqoDB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "Welcome To Colab",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
